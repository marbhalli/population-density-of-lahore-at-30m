{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f223457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import utils as smp_utils\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm as tqdm\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import glob\n",
    "import rasterio\n",
    "import json\n",
    "import re\n",
    "import tifffile\n",
    "from skimage import measure\n",
    "from rasterio.features import shapes\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d61aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#////////////////////////Pre Processing\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "\n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function\n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _transform = [\n",
    "        albumentations.Lambda(image=preprocessing_fn),\n",
    "    ]\n",
    "    return albumentations.Compose(_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc6da51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#//////////////////////// Evaluate on Large Tiff File(s) \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "#input_imgs = os.path.join(data_dir, 'tiff_images') # Input the path to tiff images\n",
    "input_imgs = r\"C:\\Users\\AhmadWaseem\\Desktop\\bhalli\\PD-Seg Work\\datasets for training - 20Z\\tiff files\\2017\\2017 reproject.tif\"\n",
    "model_path = r\"C:\\Users\\AhmadWaseem\\Desktop\\bhalli\\PD-Seg Work\\model v6 - DL 20Z - TS1024\\model h5\"\n",
    "output_dir = r\"C:\\Users\\AhmadWaseem\\Desktop\\bhalli\\PD-Seg Work\\model v6 - DL 20Z - TS1024\\output\"\n",
    "\n",
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "\n",
    "target_size = (1024, 1024)\n",
    "padding_pixels = (100, 100)\n",
    "padding_value = 0\n",
    "downsampling_factor = 1\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PIX_VALUE_MAX = 255    # The max data value we have\n",
    "PIX_VALUE_MAX_REQ = 255 # The max data value we need\n",
    "\n",
    "model = torch.load(os.path.join(model_path, 'best_model.h5'), map_location=DEVICE)\n",
    "model.eval()\n",
    "imgs = [file for file in glob.glob(input_imgs) if file.endswith('.tif')]\n",
    "print(imgs)\n",
    "assert len(imgs) > 0, \"The number of images equal to zero\"\n",
    "\n",
    "#num_processes = 1\n",
    "print(\"Running on {} images\".format(len(imgs)))# using {} parallel processes\".format(len(imgs), num_processes))\n",
    "\n",
    "# pos = len(input_imgs.split('*')[0]) #Related to naming of output file\n",
    "\n",
    "#args = [[img, DEVICE, model, args.output_dir] for img in imgs]\n",
    "\n",
    "'''\n",
    "if num_processes > 1:\n",
    "    p = mlt.Pool(num_processes)\n",
    "    (p.map(process, args))\n",
    "    p.close()\n",
    "else:\n",
    "    for arg in args:\n",
    "        process(arg)'''\n",
    "\n",
    "#img_path, device, model, output_dir, pos = args[0]\n",
    "\n",
    "for img_path in imgs:\n",
    "    #img_path = imgs[0]\n",
    "    file_name = os.path.split(img_path)[-1].split('.')[0]\n",
    "    print(\"Running for {}\".format(file_name))\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    save_path = os.path.join(output_dir, file_name + \"_preds.npy\")\n",
    "    img = np.transpose(rasterio.open(img_path).read(), (1, 2, 0))\n",
    "#     max_value = np.iinfo(img.dtype).max\n",
    "    print(\"Actual Image Size: {}\".format(img.shape))\n",
    "\n",
    "    #Define k_x, k_y to define 'useful' portion, since we are taking patches with overlapping area.\n",
    "    k_y = target_size[0] - 2 * padding_pixels[0]\n",
    "    k_x = target_size[1] - 2 * padding_pixels[1]\n",
    "\n",
    "    # First padding: To make divisible by k\n",
    "    cols = (math.ceil(img.shape[0]/k_y))\n",
    "    rows = (math.ceil(img.shape[1]/k_x))\n",
    "\n",
    "    pad_bottom = cols*k_y - img.shape[0]   #pixels to add in y direction\n",
    "    pad_right = rows*k_x - img.shape[1]    #pixels to add in x direction\n",
    "    if pad_bottom > 0 or pad_right > 0:\n",
    "        print(\"Running cv2 padding..\")\n",
    "        img = cv2.copyMakeBorder(img, 0, pad_bottom, 0, pad_right, cv2.BORDER_CONSTANT, value=padding_value)\n",
    "    print(\"Image Size after making divisible by ({}, {}): {}\".format(k_x, k_y, img.shape))\n",
    "\n",
    "    output_image = np.zeros((int(img.shape[0]*downsampling_factor), int(img.shape[1]*downsampling_factor)), dtype=np.uint8) * 255\n",
    "    print(\"Size of output image after downsampling factor of {}: {}\".format(downsampling_factor, output_image.shape))\n",
    "\n",
    "    # Second Padding: To add boundary padding pixels\n",
    "    img = cv2.copyMakeBorder(img, padding_pixels[0], padding_pixels[0], padding_pixels[1], \n",
    "                             padding_pixels[1], cv2.BORDER_CONSTANT, value=padding_value)\n",
    "    print(\"Image Size after adding ({}, {}) boundary pixels: {}\".format(padding_pixels[0], padding_pixels[0], img.shape))\n",
    "\n",
    "    # Load pre-processing function\n",
    "    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "    preprocessing = get_preprocessing(preprocessing_fn)\n",
    "\n",
    "    total_patches = rows*cols\n",
    "    print(\"Total {} patches for the given image {}\".format(rows*cols, file_name))\n",
    "    for y_idx in range(cols):\n",
    "        y1 = y_idx*k_y + padding_pixels[0]\n",
    "        y2 = y1 + k_y\n",
    "#         if y_idx <= 0:\n",
    "#             continue\n",
    "        for x_idx in range(rows):\n",
    "            x1 = x_idx*k_x + padding_pixels[1]\n",
    "            x2 = x1 + k_x\n",
    "            patch_number = y_idx*rows + x_idx + 1\n",
    "\n",
    "            img_crop = img[y1-padding_pixels[0]: y2 + padding_pixels[0], x1 - padding_pixels[1]: x2 + padding_pixels[1]]\n",
    "            print(\"Patch {} of {}: [{}:{}, {}:{}]\".format(patch_number, total_patches, y1-padding_pixels[0],\n",
    "                                                          y2 + padding_pixels[0], x1 - padding_pixels[1],\n",
    "                                                          x2 + padding_pixels[1]), end =\" \")\n",
    "            \n",
    "            img_crop = ((img_crop/PIX_VALUE_MAX)*(PIX_VALUE_MAX_REQ)).astype(np.uint8)\n",
    "#             plt.subplot(1, 2, 1)\n",
    "#             plt.imshow(img_crop)\n",
    "#             print(img_crop.max())\n",
    "            sample = preprocessing(image=img_crop)\n",
    "            image = cv2.resize(sample['image'], \n",
    "                               (int(downsampling_factor * target_size[0]),\n",
    "                                int(downsampling_factor * target_size[1])),\n",
    "                               interpolation = cv2.INTER_AREA)\n",
    "#             plt.subplot(1, 2, 2)\n",
    "#             plt.imshow(denormalize(image))\n",
    "#             plt.show()\n",
    "#             print(image.max())\n",
    "            x_tensor = torch.Tensor(image).permute(2, 0, 1).to(DEVICE).unsqueeze(0)\n",
    "#             image = np.transpose(image, (2, 0, 1))#.astype('float32')\n",
    "#             x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                pred_mask = model(x_tensor)\n",
    "            pr_mask = pred_mask.squeeze()\n",
    "            pr_mask = pr_mask.detach().squeeze().cpu().numpy().round()\n",
    "\n",
    "            patch = pr_mask[int(downsampling_factor * padding_pixels[0]) : int(downsampling_factor * (target_size[0] - padding_pixels[0])),\n",
    "                            int(downsampling_factor * padding_pixels[1]) : int(downsampling_factor * (target_size[1] - padding_pixels[1]))]\n",
    "#             print(\",Output: [{}:{}, {}:{}]\".format(int(downsampling_factor*(y_idx*k_y)), int(downsampling_factor*(y_idx*k_y + k_y)),\n",
    "#                                                    int(downsampling_factor*(x_idx*k_x)), int(downsampling_factor*(x_idx*k_x + k_x)),\n",
    "#                                                    end =\" \"))\n",
    "            output_image[int(downsampling_factor*(y_idx*k_y)): int(downsampling_factor*(y_idx*k_y + k_y)),\n",
    "                           int(downsampling_factor*(x_idx*k_x)): int(downsampling_factor*(x_idx*k_x + k_x))] = patch\n",
    "            print(\"..... Done!\")\n",
    "#         break\n",
    "\n",
    "\n",
    "output_image = output_image[:output_image.shape[0] - int(downsampling_factor * pad_bottom),\n",
    "                            :output_image.shape[1] - int(downsampling_factor * pad_right)]\n",
    "print(\"Final shape of downsampled output image: {}\".format(output_image.shape))\n",
    "\n",
    "np.save(save_path, output_image)\n",
    "\n",
    "img_array = np.load(r'C:\\Users\\AhmadWaseem\\Desktop\\bhalli\\PD-Seg Work\\model v6 - DL 20Z - TS1024\\output\\2017 reproject_preds.npy')\n",
    "tifffile.imwrite(r'C:\\Users\\AhmadWaseem\\Desktop\\bhalli\\PD-Seg Work\\model v6 - DL 20Z - TS1024\\output\\2017 reproject_preds.tiff',img_array)\n",
    "print(\"Completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
